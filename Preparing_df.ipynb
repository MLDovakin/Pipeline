{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vN2vlByXuwPG",
        "xhkxTuBgus9u",
        "vpoe8kFCugHj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "vN2vlByXuwPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p12Jm52OX6Pi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!pip install easyocr\n",
        "import easyocr\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "!pip install pdf2image\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "!apt-get install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr \\\n",
        "flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "!pip install textract\n",
        "import textract\n",
        "\n",
        "!pip install --user -U nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "!pip install tika\n",
        "from tika import parser\n",
        "\n",
        "!pip install yake\n",
        "import yake\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "xhkxTuBgus9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(type=1):\n",
        "    '''if tupe = 1, works with file. Otherwise with folder'''\n",
        "    if type == 1:\n",
        "      file_path = input(\"Enter a file path: \")\n",
        "      print(text_recognition(file_path=file_path))\n",
        "    else:\n",
        "      folder_path = input(\"Enter a folder path: \")\n",
        "      names = os.listdir(folder_path)\n",
        "      for name in names:\n",
        "        file_path = folder_path + '/' + name\n",
        "        print(text_recognition(file_path=file_path)) "
      ],
      "metadata": {
        "id": "D0ydfSwEYQNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_recognition(file_path):\n",
        "    '''For recognition pdf and convert to txt format. Also  convert doc/docx formats to txt'''\n",
        "  \n",
        "    text_file_name = f'result_{Path(file_path).stem}.txt' #name\n",
        "    \n",
        "    result = []\n",
        "    new_text = ''\n",
        "\n",
        "    if Path(file_path).suffix == '.pdf':\n",
        "      reader = easyocr.Reader([\"ru\", \"en\"]) #the best way: many pdf docs are scans with bad resolution and without text layers\n",
        "      images = convert_from_path(file_path, 500)\n",
        "      for i in range(len(images)):\n",
        "        images[i].save(f'out{i}.jpg', 'JPEG')\n",
        "        result.append(reader.readtext(f'out{i}.jpg', detail=0, paragraph=True))\n",
        "        os.remove(f'out{i}.jpg')\n",
        "      for i in range(len(result)):\n",
        "        for j in range(len(result[i])):\n",
        "          new_text = new_text+result[i][j]\n",
        "      text = text_lemma(text_preparation(new_text))\n",
        "\n",
        "      with open(text_file_name, \"w\") as file:\n",
        "        file.write(\" \".join(text))\n",
        "      \n",
        "    elif Path(file_path).suffix == '.doc':\n",
        "      parsed = parser.from_file(file_path)\n",
        "      result.append(parsed.get('content'))\n",
        "      text = text_lemma(text_preparation(result[0]))\n",
        "\n",
        "      with open(text_file_name, \"w\") as file:\n",
        "        file.write(\" \".join(text))\n",
        "\n",
        "    elif Path(file_path).suffix == '.docx':\n",
        "      text = textract.process(file_path, extension='docx').decode('utf-8')\n",
        "      text = text_lemma(text_preparation(text))\n",
        "      with open(text_file_name, \"w\") as file:\n",
        "        file.write(\" \".join(text))\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      df_creator(os.path.dirname(file_path)+'/'+text_file_name)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    return f\"Result wrote into {text_file_name}\""
      ],
      "metadata": {
        "id": "hLdi4JpAYVIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preparation(text):  \n",
        "    clean_text = text.lower().replace('_', '').replace('\\n', '').replace('(', '').replace(')', '')\\\n",
        "    .replace(\"'\", '').replace('\"', '').replace(',', '').replace('.', '').replace('-', '')\\\n",
        "    .replace('%', '').replace('№', '').replace('nг\\xa0  \\xa0\\xa0г', '').replace('1', '')\\\n",
        "    .replace('2', '').replace('3', '').replace('4', '').replace('5', '').replace('6', '')\\\n",
        "    .replace('7', '').replace('8', '').replace('9', '').replace('0', '').replace(':', '')\\\n",
        "    .replace(' п ', '').replace(';', '')\n",
        "    return  clean_text"
      ],
      "metadata": {
        "id": "qpDFzYElYa_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword(file_path):\n",
        "  with open(file_path, \"r\") as file:\n",
        "    text = file.read()\n",
        "  extractor = yake.KeywordExtractor (\n",
        "    lan = \"ru\",     # язык\n",
        "    n = 2,          # максимальное количество слов в фразе\n",
        "    dedupLim = 0.9, # порог похожести слов\n",
        "    top = 10        # количество ключевых слов\n",
        "  )\n",
        "  keywords = []\n",
        "  for i in range(len(extractor.extract_keywords(text))):\n",
        "    keywords.append(extractor.extract_keywords(text)[i][0])\n",
        "  \n",
        "  return \" \".join(keywords)"
      ],
      "metadata": {
        "id": "A24Ix0ehYr7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_lemma(text):\n",
        "  result =  filtr(lemmatize(tokeniser(text)))\n",
        "  return result"
      ],
      "metadata": {
        "id": "HJk7nXiyYot5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filtr(tokens):\n",
        "    stop_words = stopwords.words(\"russian\")\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "      if token not in stop_words:\n",
        "          filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "KzLfZz6TYhrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(tokens):\n",
        "    try:\n",
        "        wnl = nltk.WordNetLemmatizer()\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet')\n",
        "        wnl = nltk.WordNetLemmatizer()\n",
        "    return [wnl.lemmatize(t) for t in tokens] "
      ],
      "metadata": {
        "id": "TyzC8D_JYmFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokeniser(text):\n",
        "  tokens = word_tokenize(text, language=\"russian\")\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "bkz4bcmqYn8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_creator(path):\n",
        "  with open(path, \"r\") as file:\n",
        "    file_text = file.read()\n",
        "  df.loc[ len(df.index )] = [Path(path).name.split('_')[-1], file_text, keyword(path), 0]\n",
        "  return df"
      ],
      "metadata": {
        "id": "oIxMQnoTYxEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Work process\n"
      ],
      "metadata": {
        "id": "vpoe8kFCugHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['name', 'text', 'keywords', 'type'])"
      ],
      "metadata": {
        "id": "BCammmQOuBOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(type=0)"
      ],
      "metadata": {
        "id": "TTfz6O3EuJ32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}